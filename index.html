<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url(https://themes.googleusercontent.com/fonts/css?kit=OPeqXG-QxW3ZD8BtmPikfA);.lst-kix_jgwysmkiwdm7-3>li:before{content:"-  "}ul.lst-kix_winhw1uvkmki-0{list-style-type:none}.lst-kix_jgwysmkiwdm7-1>li:before{content:"-  "}.lst-kix_jgwysmkiwdm7-5>li:before{content:"-  "}ul.lst-kix_winhw1uvkmki-1{list-style-type:none}.lst-kix_jgwysmkiwdm7-0>li:before{content:"-  "}.lst-kix_jgwysmkiwdm7-4>li:before{content:"-  "}ul.lst-kix_winhw1uvkmki-4{list-style-type:none}ul.lst-kix_fry0jia86rwr-6{list-style-type:none}.lst-kix_jgwysmkiwdm7-7>li:before{content:"-  "}ul.lst-kix_winhw1uvkmki-5{list-style-type:none}ul.lst-kix_fry0jia86rwr-5{list-style-type:none}ul.lst-kix_winhw1uvkmki-2{list-style-type:none}ul.lst-kix_fry0jia86rwr-8{list-style-type:none}ul.lst-kix_winhw1uvkmki-3{list-style-type:none}ul.lst-kix_fry0jia86rwr-7{list-style-type:none}ul.lst-kix_winhw1uvkmki-8{list-style-type:none}ul.lst-kix_fry0jia86rwr-2{list-style-type:none}ul.lst-kix_fry0jia86rwr-1{list-style-type:none}ul.lst-kix_winhw1uvkmki-6{list-style-type:none}ul.lst-kix_fry0jia86rwr-4{list-style-type:none}.lst-kix_jgwysmkiwdm7-6>li:before{content:"-  "}ul.lst-kix_winhw1uvkmki-7{list-style-type:none}ul.lst-kix_fry0jia86rwr-3{list-style-type:none}ul.lst-kix_fry0jia86rwr-0{list-style-type:none}.lst-kix_jgwysmkiwdm7-8>li:before{content:"-  "}ul.lst-kix_jgwysmkiwdm7-2{list-style-type:none}ul.lst-kix_jgwysmkiwdm7-1{list-style-type:none}ul.lst-kix_jgwysmkiwdm7-0{list-style-type:none}.lst-kix_r4q5zl15uiqt-0>li:before{content:"-  "}.lst-kix_r4q5zl15uiqt-1>li:before{content:"-  "}ul.lst-kix_p1n44bntgihp-3{list-style-type:none}ul.lst-kix_p1n44bntgihp-2{list-style-type:none}ul.lst-kix_p1n44bntgihp-1{list-style-type:none}ul.lst-kix_p1n44bntgihp-0{list-style-type:none}ul.lst-kix_jgwysmkiwdm7-6{list-style-type:none}ul.lst-kix_p1n44bntgihp-7{list-style-type:none}.lst-kix_r4q5zl15uiqt-4>li:before{content:"-  "}ul.lst-kix_jgwysmkiwdm7-5{list-style-type:none}ul.lst-kix_p1n44bntgihp-6{list-style-type:none}ul.lst-kix_jgwysmkiwdm7-4{list-style-type:none}ul.lst-kix_p1n44bntgihp-5{list-style-type:none}.lst-kix_r4q5zl15uiqt-3>li:before{content:"-  "}ul.lst-kix_jgwysmkiwdm7-3{list-style-type:none}ul.lst-kix_p1n44bntgihp-4{list-style-type:none}.lst-kix_r4q5zl15uiqt-2>li:before{content:"-  "}ul.lst-kix_jgwysmkiwdm7-8{list-style-type:none}ul.lst-kix_jgwysmkiwdm7-7{list-style-type:none}ul.lst-kix_p1n44bntgihp-8{list-style-type:none}ul.lst-kix_1md9vvs52zs9-3{list-style-type:none}ul.lst-kix_1md9vvs52zs9-2{list-style-type:none}.lst-kix_fry0jia86rwr-0>li:before{content:"-  "}.lst-kix_p1n44bntgihp-8>li:before{content:"-  "}ul.lst-kix_1md9vvs52zs9-1{list-style-type:none}ul.lst-kix_1md9vvs52zs9-0{list-style-type:none}ul.lst-kix_1md9vvs52zs9-7{list-style-type:none}ul.lst-kix_1md9vvs52zs9-6{list-style-type:none}.lst-kix_fry0jia86rwr-1>li:before{content:"-  "}.lst-kix_fry0jia86rwr-2>li:before{content:"-  "}.lst-kix_p1n44bntgihp-7>li:before{content:"-  "}ul.lst-kix_1md9vvs52zs9-5{list-style-type:none}ul.lst-kix_1md9vvs52zs9-4{list-style-type:none}.lst-kix_fry0jia86rwr-3>li:before{content:"-  "}.lst-kix_fry0jia86rwr-4>li:before{content:"-  "}.lst-kix_p1n44bntgihp-4>li:before{content:"-  "}.lst-kix_p1n44bntgihp-5>li:before{content:"-  "}ul.lst-kix_1md9vvs52zs9-8{list-style-type:none}.lst-kix_p1n44bntgihp-6>li:before{content:"-  "}.lst-kix_fry0jia86rwr-7>li:before{content:"-  "}.lst-kix_p1n44bntgihp-0>li:before{content:"-  "}.lst-kix_p1n44bntgihp-1>li:before{content:"-  "}.lst-kix_fry0jia86rwr-6>li:before{content:"-  "}.lst-kix_fry0jia86rwr-8>li:before{content:"-  "}.lst-kix_fry0jia86rwr-5>li:before{content:"-  "}.lst-kix_p1n44bntgihp-3>li:before{content:"-  "}.lst-kix_p1n44bntgihp-2>li:before{content:"-  "}.lst-kix_winhw1uvkmki-8>li:before{content:"-  "}.lst-kix_winhw1uvkmki-7>li:before{content:"-  "}.lst-kix_winhw1uvkmki-6>li:before{content:"-  "}.lst-kix_winhw1uvkmki-3>li:before{content:"-  "}.lst-kix_winhw1uvkmki-5>li:before{content:"-  "}.lst-kix_winhw1uvkmki-4>li:before{content:"-  "}.lst-kix_r4q5zl15uiqt-8>li:before{content:"-  "}.lst-kix_r4q5zl15uiqt-7>li:before{content:"-  "}.lst-kix_winhw1uvkmki-1>li:before{content:"-  "}.lst-kix_winhw1uvkmki-2>li:before{content:"-  "}.lst-kix_r4q5zl15uiqt-5>li:before{content:"-  "}.lst-kix_r4q5zl15uiqt-6>li:before{content:"-  "}.lst-kix_winhw1uvkmki-0>li:before{content:"-  "}.lst-kix_1md9vvs52zs9-8>li:before{content:"-  "}.lst-kix_1md9vvs52zs9-6>li:before{content:"-  "}.lst-kix_1md9vvs52zs9-7>li:before{content:"-  "}ul.lst-kix_r4q5zl15uiqt-8{list-style-type:none}.lst-kix_1md9vvs52zs9-0>li:before{content:"-  "}.lst-kix_1md9vvs52zs9-1>li:before{content:"-  "}ul.lst-kix_r4q5zl15uiqt-6{list-style-type:none}ul.lst-kix_r4q5zl15uiqt-7{list-style-type:none}.lst-kix_1md9vvs52zs9-2>li:before{content:"-  "}ul.lst-kix_r4q5zl15uiqt-0{list-style-type:none}.lst-kix_1md9vvs52zs9-4>li:before{content:"-  "}.lst-kix_1md9vvs52zs9-5>li:before{content:"-  "}ul.lst-kix_r4q5zl15uiqt-1{list-style-type:none}ul.lst-kix_r4q5zl15uiqt-4{list-style-type:none}.lst-kix_1md9vvs52zs9-3>li:before{content:"-  "}ul.lst-kix_r4q5zl15uiqt-5{list-style-type:none}.lst-kix_jgwysmkiwdm7-2>li:before{content:"-  "}ul.lst-kix_r4q5zl15uiqt-2{list-style-type:none}ul.lst-kix_r4q5zl15uiqt-3{list-style-type:none}ol{margin:0;padding:0}table td,table th{padding:0}.c21{-webkit-text-decoration-skip:none;color:#000000;font-weight:400;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-family:"Arial";font-style:normal}.c11{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c15{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:26pt;font-family:"Arial";font-style:normal}.c8{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:22pt;font-family:"Times New Roman";font-style:normal}.c3{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Arial";font-style:normal}.c5{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c29{background-color:#ffffff;color:#202124;font-weight:400;text-decoration:none;vertical-align:baseline;font-family:"Roboto";font-style:normal}.c2{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left;height:11pt}.c27{background-color:#ffffff;-webkit-text-decoration-skip:none;color:#6611cc;font-weight:400;text-decoration:underline;text-decoration-skip-ink:none;font-family:"Roboto"}.c17{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:center}.c14{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-family:"Arial";font-style:italic}.c4{padding-top:0pt;padding-bottom:0pt;line-height:1.5;orphans:2;widows:2;text-align:left}.c19{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-family:"Times New Roman";font-style:normal}.c12{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-family:"Arial";font-style:normal}.c10{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c26{padding-top:0pt;padding-bottom:12pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c16{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c33{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-family:"Times New Roman"}.c32{background-color:#ffffff;font-family:"Roboto";color:#202124;font-weight:400}.c7{font-size:16pt;font-family:"Times New Roman";font-style:italic;font-weight:400}.c23{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c38{background-color:#ffffff;max-width:451.4pt;padding:72pt 72pt 72pt 72pt}.c36{font-family:"Times New Roman";font-weight:400}.c1{color:inherit;text-decoration:inherit}.c20{width:33%;height:1px}.c18{text-indent:36pt}.c13{height:11pt}.c34{font-size:26pt}.c31{margin-left:36pt}.c6{font-size:10pt}.c35{font-size:18pt}.c24{font-size:16pt}.c9{font-size:12pt}.c28{font-size:11pt}.c22{font-style:italic}.c37{background-color:#ffff00}.c30{font-weight:700}.c25{font-size:20pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c38 doc-content"><p class="c2"><span class="c15"></span></p><p class="c17"><span class="c19 c34">Universidad Cat&oacute;lica del Uruguay</span></p><p class="c17"><span class="c8">Facultad de Ingenier&iacute;a y Tecnolog&iacute;as</span></p><p class="c17"><span class="c8">Facultad de Ciencias Humanas</span></p><p class="c2"><span class="c19 c9"></span></p><p class="c2"><span class="c19 c9"></span></p><p class="c2"><span class="c19 c9"></span></p><p class="c2"><span class="c19 c9"></span></p><p class="c10"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 319.47px; height: 154.07px;"><img alt="" src="images/image7.png" style="width: 319.47px; height: 154.07px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c19 c9"></span></p><p class="c2"><span class="c19 c9"></span></p><p class="c2"><span class="c19 c9"></span></p><p class="c2"><span class="c19 c9"></span></p><p class="c2"><span class="c19 c9"></span></p><p class="c2"><span class="c19 c9"></span></p><p class="c2"><span class="c19 c9"></span></p><p class="c2"><span class="c9 c19"></span></p><p class="c2"><span class="c19 c9"></span></p><p class="c2"><span class="c19 c9"></span></p><p class="c2"><span class="c19 c9"></span></p><p class="c2"><span class="c19 c9"></span></p><p class="c17"><span class="c22 c25 c33">&ldquo;Tecnolog&iacute;a Deepfake Como Herramienta de Creaci&oacute;n de Contenido Audiovisual para Industrias Publicitarias&rdquo;</span></p><p class="c17 c13"><span class="c19 c25"></span></p><p class="c2" id="h.gjdgxs"><span class="c19 c25"></span></p><p class="c17 c13"><span class="c19 c25"></span></p><p class="c17"><span class="c33 c35 c22">Yves Rudi Fogel</span></p><p class="c17"><span class="c19 c35">Tutor: Ignacio Garc&iacute;a</span></p><p class="c13 c17"><span class="c19 c25"></span></p><p class="c17 c13"><span class="c19 c25"></span></p><p class="c17"><span class="c19 c24">Trabajo Final de Grado presentado como requisito parcial para la obtenci&oacute;n del grado de Licenciado en Ingenier&iacute;a Audiovisual.</span></p><p class="c17 c13"><span class="c19 c24"></span></p><p class="c17 c13"><span class="c19 c24"></span></p><p class="c16 c18"><span class="c24 c36">Montevideo, </span><span class="c7">&nbsp;18 de junio de 2020</span><hr style="page-break-before:always;display:none;"></p><p class="c16 c18"><span class="c12 c28">&Iacute;ndice</span></p><p class="c11"><span class="c5"></span></p><p class="c16"><span class="c5">Introducci&oacute;n ..........................................................................................................................p.3</span></p><p class="c16"><span class="c5">Motivaci&oacute;n ............................................................................................................................p.4</span></p><p class="c16"><span class="c5">Marco te&oacute;rico</span></p><p class="c16 c18"><span class="c5">Aprendizaje autom&aacute;tico ............................................................................................p.6</span></p><p class="c16 c18"><span>Redes Neurales</span><span class="c5">&nbsp;........................................................................................................p.7</span></p><p class="c16 c18"><span class="c5">Deepfake ..................................................................................................................p.9</span></p><p class="c16"><span>Aplicaci&oacute;n ...........................................................................................................................p.11</span></p><p class="c16"><span class="c5">Contexto</span></p><p class="c16 c31"><span>P</span><span class="c5">ublicidad ...............................................................................................................p.13</span></p><p class="c16 c31"><span class="c5">Nuevos avances .....................................................................................................p.14</span></p><p class="c16"><span class="c5">Conclusiones.......................................................................................................................p.17</span></p><p class="c16"><span>Bibliograf&iacute;a..........................................................................................................................p.18</span></p><hr style="page-break-before:always;display:none;"><p class="c11"><span class="c5"></span></p><p class="c16"><span class="c30">Introducci&oacute;n</span></p><p class="c11"><span class="c5"></span></p><p class="c4"><span class="c0">En los &uacute;ltimos a&ntilde;os la t&eacute;cnica conocida como Deepfake tuvo un crecimiento exponencial con nuevas invenciones surgiendo constantemente. Sin embargo, esta t&eacute;cnica sigue siendo vista con malos ojos, principalmente por los peligros que supone si es usada con fines perjudiciales.</span></p><p class="c4 c18"><span class="c0">M&aacute;s all&aacute; de su mala reputaci&oacute;n, estos algoritmos tienen un potencial importante ya que permiten generar contenido fotorealista de forma din&aacute;mica y a una escala de una manera como nunca antes en la historia se pudo.</span></p><p class="c4"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Mi trabajo consiste en una investigaci&oacute;n sobre c&oacute;mo este procedimiento tiene el potencial para ser una herramienta para crear valor, espec&iacute;ficamente voy a enfocarme en c&oacute;mo se puede usar para el &aacute;mbito de la publicidad.</span></p><p class="c4"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Se eligi&oacute; dar un enfoque en la publicidad por ser una industria que tiene la misi&oacute;n de estar constantemente a la vanguardia, no solo de lo nuevo, sino de lo que vendr&aacute;. Por eso es candidata para poder aprovechar los beneficios de esta tecnolog&iacute;a en el futuro pr&oacute;ximo.</span></p><p class="c4"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Con este documento se busca plantear las alternativas que se tienen al querer hacer uso de estas tecnolog&iacute;as y un plano b&aacute;sico sobre c&oacute;mo ser&iacute;a el procedimiento base para aplicarlas.</span></p><hr style="page-break-before:always;display:none;"><p class="c4 c13"><span class="c0"></span></p><p class="c4"><span class="c12 c28">Motivaci&oacute;n</span></p><p class="c4 c13"><span class="c12 c28"></span></p><p class="c4"><span class="c0">Al trabajar en una agencia de publicidad puedo ver de primera mano hacia donde se dirige la industria en Uruguay y en la regi&oacute;n. Es una industria de crecimiento r&aacute;pido pero al adentrarse uno ve ciertas dificultades en esta.</span></p><p class="c4 c18"><span class="c0">Uno de los desaf&iacute;os actuales es encontrar la forma de automatizar la creaci&oacute;n de videos. Para im&aacute;genes fijas parte del contenido que se crea para publicidad digital se hace de forma automatizada mediante programaci&oacute;n en base a datos. Esto permite generar gran cantidad de contenidos donde cada uno sea dedicado a un segmento muy espec&iacute;fico de la poblaci&oacute;n.</span></p><p class="c4 c18"><span class="c0">El futuro cercano del video en la regi&oacute;n se dirige hacia eso, poder crear una gama de videos en base a algoritmos. Hoy en d&iacute;a se est&aacute; intentando usar tecnolog&iacute;as que permitan pero su uso es muy limitado.</span></p><p class="c4 c18"><span class="c0">Otro desaf&iacute;o que tiene la industria de la publicidad en Uruguay es la dificultad de integrar nuevas tecnolog&iacute;as. La publicidad deber&iacute;a estar a la par con las vanguardias en tecnolog&iacute;a; cuando sale una nueva tecnolog&iacute;a la publicidad ya deber&iacute;a estar haciendo uso de esta. Sin embargo en estos &uacute;ltimos a&ntilde;os el mundo de la tecnolog&iacute;a creci&oacute; a un ritmo vertiginoso y a la industria publicitaria se le est&aacute; haciendo dif&iacute;cil adaptarse con la misma velocidad, por lo que no se aprovecha el potencial que ciertas nuevas tecnolog&iacute;as tienen para ofrecer.</span></p><p class="c4 c18"><span class="c0">Como estudiante de Licenciatura en Ingenier&iacute;a Audiovisual mi inter&eacute;s est&aacute; sobre todo en la uni&oacute;n de estos dos mundos: las tecnolog&iacute;as emergentes y la b&uacute;squeda de c&oacute;mo aplicarlas para fines creativos. Como estudiante tuve la fortuna de cursar una carrera donde simultaneamente cursaba materias como Laboratorio de Fotograf&iacute;a y Video Computacional y Seminario de Profundizaci&oacute;n en Arte. La primera me permiti&oacute; familiarizarme con la s&iacute;ntesis de imagen digital y entender las nuevas tecnolog&iacute;as de Visi&oacute;n Artificial; la segunda me permiti&oacute; entender que el uso de estas tecnolog&iacute;as puede ser arte y que las nuevas tecnolog&iacute;as tienen potencial como medio para ideas creativas.</span></p><p class="c4"><span class="c9">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Hace unos a&ntilde;os, cuando salieron las tecnolog&iacute;as precursoras a lo que hoy conocemos como D</span><span class="c9">eepfake</span><span class="c0">&nbsp;me ilusion&eacute; por sus posibilidades pero lo asum&iacute; como una tecnolog&iacute;a muy lejana. Al pensar en c&oacute;mo se pueden satisfacer estas tendencias en la publicidad me di cuenta que esta tecnolog&iacute;a ya est&aacute; en un grado de madurez suficiente para ser aprovechada y que logra encajar con las necesidades de la industria publicitaria. Lo &uacute;nico que se precisa es planear su aplicaci&oacute;n, lo que busca hacer este trabajo. </span></p><p class="c4 c13"><span class="c0"></span></p><p class="c4 c13"><span class="c0"></span></p><p class="c4 c13"><span class="c0"></span></p><hr style="page-break-before:always;display:none;"><p class="c4 c13"><span class="c0"></span></p><p class="c4"><span class="c12 c9">Marco Te&oacute;rico</span></p><p class="c4 c13"><span class="c12 c9"></span></p><p class="c4"><span class="c0">Para entender c&oacute;mo se aplicar&iacute;a esta tecnolog&iacute;a al mundo de la publicidad lo primero que es necesario es entender qu&eacute; es esta t&eacute;cnica. </span></p><p class="c4 c13"><span class="c0"></span></p><p class="c4"><span class="c21 c9">Aprendizaje Automatizado</span></p><p class="c4"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;En el n&uacute;cleo de esta tecnolog&iacute;a est&aacute; el Aprendizaje Automatizado, Andrew Ng (n.d.) lo describe como el &ldquo;campo de estudio que le da a las computadoras la habilidad de aprender sin ser programadas para esto&rdquo;. La base de estos sistemas es que se modifican a s&iacute; mismos en base a datos ya conocidos.</span></p><p class="c4"><span class="c9">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Lo primero que se necesita es un problema a resolver. Estos algoritmos son muy precisos para determinados problemas, pero a&uacute;n no existe y por mucho tiempo no existir&aacute; un algoritmo gen&eacute;rico que aplique a todos los casos. Por eso los algoritmos deben ser creados con un prop&oacute;sito concreto. Para ilustrar se pondr&aacute; como ejemplo un sistema que clasifique cada correo electr&oacute;nico como </span><span class="c9 c22">spam</span><span class="c9">&nbsp;o </span><span class="c14 c9">no spam.</span></p><p class="c4"><span class="c9">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Lo siguiente que se necesita es una gran cantidad de datos con sus respectivos resultados conocidos para el problema que buscamos resolver. En el clasificador de e-mails los datos ser&iacute;an una colecci&oacute;n de e-mails marcados previamente como </span><span class="c9 c22">spam</span><span class="c9">&nbsp;o </span><span class="c9 c22">no spam</span><span class="c0">. </span></p><p class="c4"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Los algoritmos de Aprendizaje Autom&aacute;tico utilizan datos conocidos, uno por uno, y intentan predecir cu&aacute;l es el resultado que tiene asignado. En base al resultado conocido se determina si est&aacute; equivocado o no. Si est&aacute; equivocado modifica sus par&aacute;metros para acercarse a la respuesta correcta, luego se pasa al siguiente dato. En nuestro ejemplo el algoritmo har&iacute;a una predicci&oacute;n en base a los e-mails sobre si es spam o no. Cuando no acierte modificar&iacute;a los par&aacute;metros hasta acercarse a la soluci&oacute;n correcta.</span></p><p class="c4"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Este proceso ser&aacute; repetido para la cantidad de datos necesarios hasta que el sistema tenga un grado de precisi&oacute;n aceptable. Una vez llegado a este punto el sistema est&aacute; listo para recibir datos nuevos de los cuales no sabemos su respuesta y predecir esta con un peque&ntilde;o grado de error.</span></p><p class="c4"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Existen varios algoritmos que entran en la clasificaci&oacute;n de Aprendizaje Autom&aacute;tico. Para este trabajo nos vamos a centrar en uno en espec&iacute;fico: las Redes Neurales.</span></p><p class="c4 c13"><span class="c0"></span></p><p class="c4"><span class="c9 c21">Redes Neurales</span></p><p class="c4"><span class="c0">Estos son algoritmos que se inspiran en la interconexi&oacute;n de neuronas en un cerebro humano para dise&ntilde;ar su arquitectura. Son especialmente usados en tareas de clasificaci&oacute;n y agrupaci&oacute;n de datos ya que son efectivos para reconocer patrones.</span></p><p class="c4"><span class="c9">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Los algoritmos de redes neurales son tambi&eacute;n llamados aproximadores universales porque aprenden a aproximar cualquier funci&oacute;n </span><img src="images/image1.png"><span class="c9">&nbsp;entre datos de entrada </span><img src="images/image2.png"><span class="c9">&nbsp;y de salida </span><img src="images/image3.png"><span class="c9">. En la etapa de aprendizaje, la red aprende qu&eacute; forma tiene esta funci&oacute;n </span><img src="images/image4.png"><span class="c9">&nbsp;en base a muchos pares de </span><img src="images/image2.png"><span class="c9">&nbsp;e </span><img src="images/image3.png"><span class="c9">. Una vez que est&aacute; entrenado, este sistema recibe un valor de entrada y retorna el resultado de esta </span><img src="images/image5.png"><span class="c0">.</span></p><p class="c4"><span class="c9">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;La arquitectura de este sistema se basa en capas, la unidad b&aacute;sica de cada capa siendo un nodo. Esto es simplemente un recipiente donde se colocar&aacute; un valor, un n&uacute;mero normalizado. Cada uno de estos nodos influye en el valor que tendr&aacute; cada nodo de la capa siguiente, llamaremos a esta influencia una </span><span class="c9 c22">conexi&oacute;n</span><span class="c0">. Si fu&eacute;ramos a representar gr&aacute;ficamente un sistema simple de este estilo ser&iacute;a de la siguiente manera:</span></p><p class="c4"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 437.33px;"><img alt="" src="images/image6.png" style="width: 602.00px; height: 437.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4"><span class="c14 c9">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Figura 1: Representaci&oacute;n gr&aacute;fica de una Red Neural</span></p><p class="c4 c18 c13"><span class="c0"></span></p><p class="c4 c18"><span class="c0">El sistema en la figura 1 tiene 4 capas: la primera, de 3 nodos, se llama la capa de entrada. La &uacute;ltima capa, de 2 nodos, se llama la capa de salida. Las capas entre medio se llaman capas ocultas, en este caso tienen 4 nodos cada una. </span></p><p class="c4 c18"><span class="c0">Cada conexi&oacute;n, representada por una l&iacute;nea uniendo nodos, tiene un peso asociado. En este caso entre la capa 1 y 2 hay 12 pesos, entre la 2 y 3 hay 16 pesos, entre la 3 y 4 hay 8 pesos, o sea que en total hay 36 pesos distintos. </span></p><p class="c4 c18"><span class="c9">El &uacute;ltimo elemento para completar la arquitectura son los llamados </span><span class="c9 c22">biases</span><span class="c0">, estos funcionan como si fueran un nodo m&aacute;s en cada capa pero que no dependen de los valores de entrada sino que son valores constantes, si como dijimos una red neural es un aproximador universal, estos biases son el t&eacute;rmino independiente de la ecuaci&oacute;n.</span></p><p class="c4 c18"><span class="c0">Una red neural no es m&aacute;s que una serie de sumas ponderadas donde distintos valores de entrada se multiplican por sus pesos correspondientes y luego son sumados para generar el siguiente nodo. Esto se har&aacute; para todos los nodos de la siguiente capa y estos nodos ser&aacute;n los valores intermedios. Se repetir&aacute; el proceso para ir generando las siguientes capas hasta que se llega al valor de salida. Finalmente los nodos de salida tendr&aacute;n distintos valores y se tomar&aacute; el valor mayor como el valor de la salida buscado.</span></p><p class="c4 c18"><span class="c9">Para que todo esto funcione correctamente debemos determinar el valor de los pesos y los </span><span class="c9">biases</span><span class="c9">&nbsp;de la red. Eso es el rol de la etapa de aprendizaje. Mediante una t&eacute;cnica llamada </span><span class="c9 c22">backpropagation</span><span class="c0">&nbsp;se recorre de manera inversa la red: partiendo de que sabemos los valores de entrada y de salida buscamos ajustar los pesos y biases para que satisfagan esos valores. Se repite el proceso para miles de datos para que cada vez sea m&aacute;s acertado el valor.</span></p><p class="c4 c18"><span class="c9">Cuando esta red ya est&aacute; entrenada, por ende est&aacute;n ajustados los pesos y biases, podemos ingresar datos de entrada y nos devolver&aacute; un resultado de salida que idealmente sea correcto.</span></p><p class="c4 c18"><span class="c0">Esta es la arquitectura base de todas las redes neurales, sin embargo la t&eacute;cnica ha ido avanzando y existen algoritmos de redes neurales especializadas en distintas tareas. Una de ellas es la Red Neural Convolucional que funciona especialmente bien para procesamiento de im&aacute;genes. Estas redes, como dice su nombre se basan en la operaci&oacute;n matem&aacute;tica de convoluci&oacute;n y es una tecnolog&iacute;a muy usada para detecci&oacute;n de objetos, identificaci&oacute;n facial e incluso se usa para veh&iacute;culos aut&oacute;nomos.</span></p><p class="c4 c13"><span class="c0"></span></p><p class="c4"><span class="c21 c9">Deepfake</span></p><p class="c4 c18"><span class="c9">El objetivo de las tecnolog&iacute;as </span><span class="c9">deepfake</span><span class="c9">&nbsp;es generar un video sint&eacute;tico realista usando inteligencia artificial. Para esto se toma un video de una persona A y se superponen los gestos de una segunda persona B. Esto se logra en base a unas redes </span><span class="c9">neurales</span><span class="c0">&nbsp;convolucionales llamadas encoder-decoder.</span></p><p class="c4 c18"><span class="c9">Estas redes reciben de entrada una imagen y su salida es la misma imagen. La utilidad est&aacute; en que en su arquitectura en el centro tienen una capa con un n&uacute;mero peque&ntilde;o de nodos, por lo que reducen la imagen a sus elementos m&aacute;s fundamentales, con la porci&oacute;n llamada encoder, y en base a estos elementos </span><span class="c9">reconstruyen</span><span class="c0">&nbsp;la imagen, con la porci&oacute;n llamada decoder.</span></p><p class="c4 c18"><span class="c0">En el caso de los Deepfake se entrena un encoder-decoder con im&aacute;genes de la persona A, por lo que sabe c&oacute;mo deconstruir y reconstruir estas im&aacute;genes y otro algoritmo con imagenes de la persona B.</span></p><p class="c4 c18"><span class="c0">Al momento de hacer el Deepfake para cada frame del video el algoritmo toma de entrada ese frame que es una imagen que tiene a la persona A, deconstruye esa imagen con el encoder pero al reconstruirla utiliza el decoder de la persona B. El resultado de esto son im&aacute;genes realistas donde los gestos de la persona A se suplantan por los de la persona B.</span></p><p class="c4 c18 c13"><span class="c0"></span></p><hr style="page-break-before:always;display:none;"><p class="c11"><span class="c5"></span></p><p class="c16"><span class="c12 c9">Aplicaci&oacute;n</span></p><p class="c11"><span class="c5"></span></p><p class="c4"><span class="c0">Los algoritmos de Deepfake representan la vanguardia en generaci&oacute;n de im&aacute;genes sint&eacute;ticas. Es una tecnolog&iacute;a que mejora pronunciadamente mes a mes tanto en realismo como en la velocidad de creaci&oacute;n. Sin embargo su aplicaci&oacute;n a&uacute;n no logra despegar y las creaciones son sobre todo demostraciones de la t&eacute;cnica.</span></p><p class="c4"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Existe sin embargo un &aacute;rea donde su uso puede ser real: la industria publicitaria. Este trabajo busca delimitar una gu&iacute;a para el uso de esta tecnolog&iacute;a hoy en d&iacute;a en el &aacute;mbito de la publicidad como forma de generar contenidos.</span></p><p class="c4"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Antes de detallar c&oacute;mo se usar&iacute;a esta tecnolog&iacute;a, resulta de suma importancia destacar que se tiene que tener especial consideraci&oacute;n por las cuestiones &eacute;ticas. En primer lugar, como destaca Sloane (2019), toda persona cuya imagen sea sujeta a este tipo de s&iacute;ntesis debe ser conciente en todo momento de lo que se har&aacute; con su imagen y debe estar de acuerdo con ello. En segundo lugar, la audiencia debe ser advertida de que las im&aacute;genes que est&aacute; viendo han sido manipuladas. Como dice el nombre, los Deepfake han ganado popularidad como herramientas para difundir noticias falsas por lo que es esencial ser transparente en su uso y tener consideraci&oacute;n para con la gente.</span></p><p class="c4"><span class="c9">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Para su uso pr&aacute;ctico existen softwares que permiten crear videos con Deepfake sin tener que crear una infraestructura desde cero. Los ejemplos m&aacute;s conocidos de estos programas son FakeApp, FaceSwap</span><sup class="c9"><a href="#ftnt1" id="ftnt_ref1">[1]</a></sup><span class="c9">&nbsp;y DeepFaceLab</span><sup class="c9"><a href="#ftnt2" id="ftnt_ref2">[2]</a></sup><span class="c0">. Por ser el programa m&aacute;s vers&aacute;til se usar&aacute; este &uacute;ltimo para el trabajo.</span></p><p class="c4"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;La primer aplicaci&oacute;n de este programa para la industria de la publicidad es la que se le viene a la mente a la gente al hablar de Deepfake; cambiarle la cara o la cabeza a un personaje por otro de una manera creible y fotorealista.</span></p><p class="c4"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;La publicidad se basa cada vez m&aacute;s en videos y cada producci&oacute;n de video es un problema log&iacute;stico importante donde se requiere de mucho dinero, gente y tiempo. Si, por ejemplo, se decide que se quiere hacer la misma publicidad con otro actor para apelar a otra demograf&iacute;a hay que volver a hacer todo ese proceso.</span></p><p class="c4 c18"><span class="c0">En casos de esa &iacute;ndole entra en juego esta tecnolog&iacute;a donde se pueden crear una variedad de publicidades con distintos actores sin precisar m&aacute;s que una selecci&oacute;n de fotos obtenidas de internet. Esta t&eacute;cnica precisar&iacute;a de un doblaje para la voz o t&eacute;cnicas an&aacute;logas que modifiquen el timbre de las voces. Un solo rodaje puede ser aplicado a los actores que se desee, esto permite que el mensaje llegue a audiencias que sin esta t&eacute;cnica no lo har&iacute;a.</span></p><p class="c4 c18"><span class="c0">Un grado de complejidad m&aacute;s ser&iacute;a usar una t&eacute;cnica dentro del conjunto de las Deepfakes llamada Lip Sync. Esta t&eacute;cnica recibe un video de una persona y un audio y el resultado es la manipulaci&oacute;n de la cara de la persona en el video para que parezca que est&aacute; diciendo lo del audio.</span></p><p class="c4 c18"><span class="c0">El alcance de esta t&eacute;cnica es mayor pero su desarrollo no est&aacute; a&uacute;n a la altura de la t&eacute;cnica mencionada anteriormente. Si se logra usar efectivamente, este algoritmo permitir&iacute;a que con una pieza audiovisual base se pueda generar diversos mensajes para diversas audiencias pr&aacute;cticamente sin costo. </span></p><p class="c4 c18"><span class="c0">El proceso pr&aacute;ctico para usar ambas t&eacute;cnicas es similar. En primer lugar se precisan cientos de fotos de ambos personajes: el que est&aacute; en el video y el que se quiere que est&eacute; en el video. El software toma estas fotos y entrena 2 modelos con pares de encode-decoders que pueden deconstruir y reconstruir fotos de las 2 personas. El siguiente paso es ingresar el video al programa y este dar&aacute; en su salida el mismo video pero con la persona cambiada.</span></p><hr style="page-break-before:always;display:none;"><p class="c4 c18 c13"><span class="c0"></span></p><p class="c16"><span class="c12 c9">Contexto</span></p><p class="c11"><span class="c12 c9"></span></p><p class="c16"><span class="c9 c30">Publicidad</span></p><p class="c4"><span class="c0">La industria publicitaria, especialmente en lo que refiere a la creaci&oacute;n de contenido, necesita estar siempre cambiando para adaptarse a su audiencia, por esta raz&oacute;n las tendencias en la publicidad var&iacute;an constantemente. En Uruguay y en la regi&oacute;n existen varias tendencias que sirven para ver la direcci&oacute;n hacia donde se dirige la industria.</span></p><p class="c4 c18"><span class="c0">En primer lugar la publicidad se enfoca cada vez m&aacute;s en el mundo digital. Dentro de la publicidad es el sector que m&aacute;s crece y dentro de ese sector el crecimiento est&aacute; fuertemente enfocado en todo lo que es mobile. Las redes sociales cumplen un rol sustancial en esto.</span></p><p class="c4 c18"><span class="c0">La segunda tendencia es una b&uacute;squeda de publicidades personalizadas. Las tecnolog&iacute;as en internet permiten a las agencias de publicidad tener una gran cantidad de datos de cada persona que est&aacute; accediendo a cada p&aacute;gina en cada momento. Los estudios muestran que el segmentar la audiencia y hacer publicidades especializadas en determinados sectores de la poblaci&oacute;n el impacto de la publicidad se multiplica. Esto da lugar a tecnolog&iacute;as como la compra program&aacute;tica que permite a las agencias determinar mediante algoritmos si es rentable comprar un espacio publicitario para esa persona determinada en ese momento, todo en cuesti&oacute;n de milisegundos. Para aumentar la personalizaci&oacute;n de publicidades se usa una herramienta llamada Dynamic Creative Optimization (DCO) que mediante un programa de computadora y con los datos que se sabe de la audiencia genera publicidades est&aacute;ticas dise&ntilde;adas especialmente para la persona que est&aacute; viendo el anuncio.</span></p><p class="c4 c18"><span class="c9">Para publicidad est&aacute;tica estas tecnolog&iacute;as se usan en masa, pero en cuanto a </span><span class="c9">video</span><span class="c0">&nbsp;se abre un nicho. Existen soluciones basadas sobre todo en animaciones b&aacute;sicas pero a&uacute;n no hay soluciones dominantes para generar videos de forma din&aacute;mica que permita personalizaci&oacute;n. </span></p><p class="c4 c18"><span class="c0">La tercer tendencia es referida a la generaci&oacute;n de contenidos, sobre todo en video donde es m&aacute;s importante generar muchos contenidos distintos aunque sean sencillos que contenidos con mucha producci&oacute;n. Esta tendencia se potencia con el uso masivo de las redes sociales donde cobra m&aacute;s importancia generar contenido nuevo constantemente para mantenerse relevante.</span></p><p class="c4 c18"><span class="c0">El procedimiento descrito en este trabajo ayudar&iacute;a a estas empresas a mantenerse actualizado con estas tres tendencias. Poder generar videos con distintos personajes sin tener que realizar rodajes costosos en dinero y en tiempo permite generar de manera r&aacute;pida y barata videos que apuntan a segmentos muy detallados de la demograf&iacute;a, haciendo que el mensaje sea m&aacute;s efectivo. Si m&aacute;s all&aacute; de cambio de persona se agrega la t&eacute;cnica de Lip Sync se puede hacer que estos mensajes sean incluso m&aacute;s efectivos para cada grupo demogr&aacute;fico. </span></p><p class="c4 c18"><span class="c9">A modo de ejemplo suponga que se es una empresa que vende bebidas. En el estado de la publicidad digital hoy se puede dividir la demograf&iacute;a en sectores etarios, en sectores basados en ubicaci&oacute;n, en gustos determinados y en una serie de factores m&aacute;s. Si quisi&eacute;ramos hacer una publicidad basada en celebridades se </span><span class="c9">precisar&iacute;an</span><span class="c0">&nbsp;decenas de celebridades para satisfacer a estos grupo, por lo que se tendr&iacute;an dos opciones: realizar decenas de rodajes para cada celebridad o apuntar a algunos segmentos demogr&aacute;ficos y dejar de lado a los otros. La t&eacute;cnica propuesta permite con un solo rodaje poder apuntar a todos estos sectores por m&aacute;s espec&iacute;ficos que sean.</span></p><p class="c4 c18 c13"><span class="c0"></span></p><p class="c16"><span class="c12 c9">Nuevos Avances</span></p><p class="c4"><span class="c9">Las t&eacute;cnicas de </span><span class="c9">Deepfake</span><span class="c0">&nbsp;son resultado de las &uacute;ltimas novedades en el &aacute;rea de visi&oacute;n computarizada, un &aacute;rea que avanza a ritmo acelerado. Existen distintas innovaciones que actualmente se est&aacute;n llevando a cabo que puede hacer que esta t&eacute;cnica d&eacute; el siguiente paso a su implementaci&oacute;n masiva.</span></p><p class="c4"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;La primera son los desarrollos en Generative Adversarial Networks. Estas son un tipo de red neural relativamente nuevo que, a medida que avancen sus implementaciones, van a llevar a los Deepfake a nuevos niveles de realismo y de versatilidad.</span></p><p class="c4"><span class="c9">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;En enero de este a&ntilde;o, Justus Thies present&oacute; un art&iacute;culo llamado &ldquo;</span><span class="c9 c22">Neural Voice Puppetry: Audio-driven Facial Reenactment</span><span class="c0">&rdquo; (Thies, 2020). A grandes rasgos presenta un sistema que permite crear Deepfake simplemente en base a audio de gente hablando. Si se perfecciona lo propuesto en este art&iacute;culo se podr&iacute;a llevar a cabo lo propuesto sin tener que llevar a cabo ni un rodaje, simplemente teniendo fotos de los personajes y un video gen&eacute;rico base.</span></p><p class="c4"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Existe una tercer t&eacute;cnica para terminar de darle un futuro prometedor, casi ut&oacute;pico a la tecnolog&iacute;a, son los sistemas donde, en base a inteligencia artificial, reciben como datos de entrada un texto y producen un sonido humano realista. El l&iacute;der en el desarrollo de estas tecnolog&iacute;as es Google con una empresa llamada WaveNet. </span></p><p class="c4"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Usando las tecnolog&iacute;as ya disponibles y combinandolas con estas mencionadas, en un futuro cercano se podr&iacute;a generar un software que en base simplemente a texto pueda crear videos publicitarios &uacute;nicos desarrollados especialmente para la persona mir&aacute;ndolos. Estos videos ser&iacute;an creados casi instant&aacute;neamente a medida que los procesadores mejoren por lo que los mismos videos podr&iacute;an ir variando a medida que se conoce m&aacute;s de la persona, simplemente viendo su forma de navegar la red.</span></p><p class="c4"><span class="c9">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Anteponiendose al uso masivo de estas tecnolog&iacute;as habr&iacute;a que llevar a cabo una investigaci&oacute;n de los usuarios sobre c&oacute;mo responden a eso. Ya se han llevado a cabo estudios que indican que, en la mayor parte de los casos, los humanos no pueden diferenciar videos alterados mediante Deepfake de videos originales, sin embargo estos estudios no indican cual es la respuesta subconsciente de los humanos a estos videos. A medida que se asemejan a </span><span class="c9">videos</span><span class="c9">&nbsp;originales en los ojos humanos, se corre el riesgo de adentrarse en lo que Masahiro Mori (1970) llam&oacute; &ldquo;</span><span class="c9 c22">Valle Inquietante</span><span class="c0">&rdquo;. Este es un concepto te&oacute;rico que dice que a medida que los robots antropom&oacute;rficos se aproximan en apariencia a los humanos, existe un punto donde la respuesta emocional del humano hacia el robot decae abruptamente.</span></p><p class="c4"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Para evitar que este sea el caso para Deepfakes se propone un experimento: generar una serie de videos publicitarios basados en estos algoritmos, presentarlos a una serie de participantes y medir su respuesta. Habr&iacute;a que contrastar estos resultados con un grupo de control al que se le muestren videos creados con humanos para medir el cambio en los resultados y as&iacute; aproximar el impacto que tiene en los individuos el uso de Deepfake en videos. Es necesario llevar a cabo estas investigaciones antes de presentar las pautas en el mercado para evitar repercusiones negativas innecesarias.</span></p><p class="c4"><span class="c9">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Existe sin embargo una serie de innovaciones en el &aacute;rea que son incluso m&aacute;s importantes que todas estas. Hasta este punto en el trabajo se parti&oacute; de la hip&oacute;tesis de que estas herramientas ser&iacute;an usadas de una forma &eacute;tica y responsable. Lamentablemente no existen garant&iacute;as de que se vayan a usar as&iacute; y existen antecedentes para desconfiar. Por eso es de suma importancia trabajos como el llevado a cabo en el art&iacute;culo &ldquo;FaceForensics++: Learning to Detect Manipulated Facial Images&rdquo; (Rosslet et al, 2019). Estos sistemas son similares en arquitectura a los que generan Deepfakes pero su uso es opuesto, utilizan redes </span><span class="c9">neurales</span><span class="c9">&nbsp;en arquitectura GAN para detectar videos que hayan sido manipulados. Los resultados no son perfectos pero son bastante mejor detectando videos manipulados que los ojos humanos. Este tipo de tecnolog&iacute;a comenz&oacute; una especie de juego de gato y rat&oacute;n donde la gente que crea los </span><span class="c9">videos</span><span class="c0">&nbsp;manipulados intenta mejorar sus algoritmos para burlar los de detecci&oacute;n y la gente que crea algoritmos de detecci&oacute;n los mejora para incluir estos cambios en sus detecciones.</span></p><p class="c4 c18"><span class="c0">Resulta importante reiterar que en este trabajo se le pide a la gente que quiera usar este tipo de tecnolog&iacute;as que lo haga desde un punto de vista &eacute;tico y humano, con total consentimiento de la persona cuya imagen se modificar&aacute; y poniendo valores humanos primero.</span></p><p class="c4 c13"><span class="c0"></span></p><hr style="page-break-before:always;display:none;"><p class="c4 c13"><span class="c0"></span></p><p class="c4"><span class="c12 c9">Conclusiones</span></p><p class="c4"><span class="c0">Las nuevas maneras de generar contenido de manera masiva van a generar una disrupci&oacute;n en el mundo de la publicidad, es simplemente una cuesti&oacute;n de tiempo. La tecnolog&iacute;a a&uacute;n no est&aacute; suficientemente madura para poder ser usada en todos los &aacute;mbitos pero s&iacute; para sus primeras implementaciones.</span></p><p class="c4"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Por esta raz&oacute;n, es momento de que se d&eacute; un debate sobre el uso de estas tecnolog&iacute;as para determinar qu&eacute; corresponde un uso &eacute;tico y c&oacute;mo evitar que su uso corrompa la visi&oacute;n que se tiene sobre la verdad.</span></p><p class="c4"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;M&aacute;s all&aacute; de eso, lo desarrollado en el trabajo tiene un uso pr&aacute;ctico real hoy en d&iacute;a que cada vez va a servir m&aacute;s. Por supuesto que se va a tener que adaptar a medida que las tecnolog&iacute;as avancen para estar a la altura de los avances tecnol&oacute;gicos.</span></p><p class="c4"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Lo cierto es que la disrupci&oacute;n de la inteligencia artificial en el mundo audiovisual est&aacute; en camino, esta es simplemente una de sus formas. Como miembros de la industria audiovisual nuestra respuesta no deber&iacute;a ser rechazar las nuevas formas de generar contenido sino sumarlas como una herramienta m&aacute;s en nuestro poder para hacer nuestro trabajo m&aacute;s efectivo.</span></p><p class="c4"><span class="c9">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;En el mundo de la publicidad estas nuevas herramientas van a permitir que los mensajes de los auspiciantes lleguen a audiencias nuevas de maneras m&aacute;s efectiva. Sin embargo la tecnolog&iacute;a sola no logra mucho; una vez que estos procedimientos se utilicen masivamente y saturen el mercado va a depender, como hoy en dia, de la creatividad de los publicistas y de las estrategias bien logradas el poder hacer que un anuncio destaque.</span><hr style="page-break-before:always;display:none;"></p><p class="c4"><span class="c9 c12">Bibliograf&iacute;a</span></p><p class="c10"><span class="c6">Suwajanakorn, S., Seitz, S., &amp; Kemelmacher-Shlizerman, I. (2017). </span><span class="c14 c6">Synthesizing</span></p><p class="c10"><span class="c6 c22">Obama: Learning Lip Sync from Audio</span><span class="c3">.</span></p><p class="c2"><span class="c3"></span></p><p class="c10"><span class="c6">Thies, J., Zollh &#776;ofer, M., Stamminger, M., Theobalt, C., &amp; Nie&szlig;ner, M. (2016). </span><span class="c14 c6">Face2Face: Real-TimeFace Capture and Reenactment of RGB Videos.</span></p><p class="c2"><span class="c3"></span></p><p class="c10"><span class="c6">Nguyen, T., Nguyen, C., Nguyen, D., Nguyen, D. &amp; Nahavandi, S. (2019). </span><span class="c14 c6">Deep Learning for Deepfakes Creation and Detection</span></p><p class="c2"><span class="c3"></span></p><p class="c10"><span class="c6">Rossler, A., Cozzolino, D., Verdoliva, L., Riess, C., Thies, J. &amp; Nie&szlig;ner, M. (2019). </span><span class="c14 c6">Faceforensics++: Learning to detect manipulated facial images. </span></p><p class="c2"><span class="c3"></span></p><p class="c10"><span class="c6">Perov I., Gao D., Chervoniy N., Liu K., Marangonda S., Um&eacute; C., Mr. Dpfks, Facenheim, C., Luis RP, Jiang, J., Zhang, S., Wu, P., Zhou, B. &amp; Zhang, W. (2020). </span><span class="c14 c6">DeepFaceLab: A simple, flexible and extensible faceswapping framework</span></p><p class="c2"><span class="c3"></span></p><p class="c10"><span class="c6">Kingma, D. &amp; Welling, M. (2014). </span><span class="c6 c14">Auto-Encoding Variational Bayes</span></p><p class="c2"><span class="c3"></span></p><p class="c26"><span class="c6">Rungta, K. (2018). </span><span class="c6 c22">TensorFlow in 1 day: Make your own neural network</span><span class="c3">.</span></p><p class="c26"><span class="c6">Ng, A. (n.d.). Machine Learning [Curso Online]. Coursera. </span><span class="c6 c23"><a class="c1" href="https://www.google.com/url?q=https://www.coursera.org/lecture/machine-learning/welcome-to-machine-learning-zcAuT&amp;sa=D&amp;source=editors&amp;ust=1724708615411507&amp;usg=AOvVaw1RFkxYuGA_uT0wPvqpP_4H">https://www.coursera.org/lecture/machine-learning/welcome-to-machine-learning-zcAuT</a></span></p><p class="c26"><span class="c6">Viskovich, J. (2020). </span><span class="c6 c22">Why Hyper-Personalization is Key for Marketers in 2020</span><span class="c6">. Recuperado el 17 de junio de 2020, de </span><span class="c23 c6"><a class="c1" href="https://www.google.com/url?q=https://www.socialmediatoday.com/news/why-hyper-personalization-is-key-for-marketers-in-2020/570276/&amp;sa=D&amp;source=editors&amp;ust=1724708615412135&amp;usg=AOvVaw1E3CPyZSUI8BnvAIc4WPK3">https://www.socialmediatoday.com/news/why-hyper-personalization-is-key-for-marketers-in-2020/570276/</a></span></p><p class="c26"><span class="c6">Hui, J. (2020). </span><span class="c6 c22">How deep learning fakes videos (Deepfake) and how to detect it?</span><span class="c6">&nbsp;Recuperado el 18 de junio de 2020, de </span><span class="c23 c6"><a class="c1" href="https://www.google.com/url?q=https://medium.com/@jonathan_hui/how-deep-learning-fakes-videos-deepfakes-and-how-to-detect-it-c0b50fbf7cb9&amp;sa=D&amp;source=editors&amp;ust=1724708615412659&amp;usg=AOvVaw2rtl4VXj-LEVbuKOtCYL4E">https://medium.com/@jonathan_hui/how-deep-learning-fakes-videos-deepfakes-and-how-to-detect-it-c0b50fbf7cb9</a></span></p><p class="c26"><span class="c6">Sloane, G. (2019). </span><span class="c6 c22">What marketers need to know about deepfakes</span><span class="c6">. Recuperado el 18 de junio de 2020, de </span><span class="c23 c6"><a class="c1" href="https://www.google.com/url?q=https://adage.com/article/digital/what-marketers-need-know-about-deepfakes/2206586&amp;sa=D&amp;source=editors&amp;ust=1724708615413042&amp;usg=AOvVaw1_o996bn_M72LxvIKBB2iT">https://adage.com/article/digital/what-marketers-need-know-about-deepfakes/2206586</a></span><span class="c3">&nbsp;</span></p><p class="c26"><span class="c6">Masahiro Mori (1970). The Uncanny Valley (K. MacDorman, Trans.) Recuperado el 18 de junio de 2020 de </span><span class="c23 c6"><a class="c1" href="https://www.google.com/url?q=https://spectrum.ieee.org/automaton/robotics/humanoids/the-uncanny-valley&amp;sa=D&amp;source=editors&amp;ust=1724708615413420&amp;usg=AOvVaw0JvCHUaqqdXlYUSvRON61n">https://spectrum.ieee.org/automaton/robotics/humanoids/the-uncanny-valley</a></span><span class="c3">&nbsp;</span></p><p class="c26"><span class="c6">Mathura, M. &amp; Reichling, D. (2016). </span><span class="c6 c22">Navigating a social world with robot partners: A quantitative cartography of the Uncanny Valley</span><span class="c6">&nbsp;</span></p><div><p class="c11"><span class="c5"></span></p></div><hr class="c20"><div><p class="c10"><a href="#ftnt_ref1" id="ftnt1">[1]</a><span class="c6">&nbsp;</span><span class="c9 c32">https://faceswap.dev/</span></p></div><div><p class="c10"><a href="#ftnt_ref2" id="ftnt2">[2]</a><span class="c6">&nbsp;</span><span class="c9 c27"><a class="c1" href="https://www.google.com/url?q=https://github.com/iperov/DeepFaceLab&amp;sa=D&amp;source=editors&amp;ust=1724708615414160&amp;usg=AOvVaw1BkZfj7-YkLTJ7MLuOMdYt">https://github.com/iperov/DeepFaceLab</a></span></p></div></body></html>